# Общий путь пакета «с нуля»

_Возьмём пример: пакет прилетает в сетевую карту (NIC) → Linux ядро → приложение._

## Входящий пакет

### 1. NIC (сетевой адаптер)

**NIC (Network Interface Card)** — это твоя сетевая карта (может быть встроена в материнку).\
Она умеет:

- принимать Ethernet-кадры (кадр = данные + заголовки MAC/IP/TCP и т.д.); 
- сама вычислять контрольные суммы (offload); 
- сама резать/склеивать пакеты (TSO/GSO/GRO); 
- писать принятые кадры в память сервера.

### 2. DMA (Direct Memory Access)

Обычно устройства не нагружают CPU копированием данных.\
Вместо этого используется DMA — механизм, при котором железо напрямую пишет данные в оперативную память (RAM), минуя процессор.


**Для сетевой карты это значит:**\
Прилетел пакет → NIC кладёт его в выделенный участок RAM (ring buffer).\
CPU вообще не вмешивается в копирование.

**Ring buffer** = кольцевой буфер: массив ячеек памяти, в которые по кругу складываются пакеты.

### 3. IRQ (Interrupt Request — аппаратное прерывание)

После того как NIC положил пакет в память через DMA, он должен сигнализировать процессору: «у меня есть новый пакет».

Это делается через IRQ (interrupt request, прерывание):
- У каждого устройства есть «линия прерывания». 
- NIC дёргает её → CPU прерывает выполнение текущих задач → вызывает драйвер сетевой карты.

**Но!**\
Если пакетов очень много (например, 10 млн/сек), то дёргать CPU на каждый пакет = перегрузка.

### 4. NAPI (New API)

Чтобы снизить нагрузку от IRQ, в Linux придумали NAPI.

Принцип:

- NIC вызывает прерывание только один раз («у меня есть данные»).
- Драйвер переводит устройство в режим polling (опроса).
- CPU сам заходит и забирает пачку пакетов из DMA буфера (например, 64 сразу).

Так снижается количество прерываний и улучшается производительность.

### 5. SKB (struct sk_buff)

Когда драйвер забрал пакет из DMA буфера, он превращает его в объект ядра — skb (socket buffer).\
Это структура данных в Linux, которая хранит информацию о пакете: заголовки, длину, метки (fwmark), принадлежность к conntrack и т.д.

Все пакеты в ядре «живут» как skb.

### 6. Дальше в ядре (networking stack)

Теперь, когда пакет в skb, ядро решает:

- куда его отправить (маршрутизация); 
- фильтровать ли его (netfilter, firewall); 
- считать ли его «новым соединением» (conntrack); 
- какой сокет ждёт этот пакет (TCP/UDP/RAW).

**_Подробнее:_**

После того как драйвер поднял пакет из DMA в память и создал объект skb (socket buffer), начинается жизнь пакета внутри ядра.\
Этот путь обычно делят на ingress (вход) и egress (выход), с разными "хуками" (точками обработки).

**A) Ingress (входящий пакет)**

1. Получение skb
- Драйвер вызывает netif_receive_skb() или napi_gro_receive().
- Пакет теперь — полноценный объект struct sk_buff.
2. Traffic Control (ingress qdisc)
- Здесь может быть фильтрация/shape через tc filter add ... ingress.
- По умолчанию пусто.
3. Netfilter hook: PREROUTING
- Первое место, где вступают в игру iptables/nftables:
  - NAT (DNAT), mangle, raw (для conntrack bypass).\
  _Пример: сменить адрес назначения или порт._
4. Routing (FIB lookup)\
Ядро смотрит таблицы маршрутизации (ip route).\
Решает:
- Этот IP принадлежит локальному хосту → обработать локально.
- Или надо переслать дальше → FORWARD.
5. Если локально
- Пакет идёт в цепочку INPUT (netfilter).
- Проверяются firewall правила.
- Если разрешено — пакет доставляется в socket lookup:\
_Ядро ищет сокет, который слушает (dstIP, dstPort, proto)._
6. Доставка в сокет
- TCP/UDP стек принимает skb.
- Для TCP: проверки ACK, retransmit, congestion control. 
- Для UDP: сразу в очередь сокета.
- Когда приложение делает recv(), данные копируются из socket buffer в userspace.

**B) Egress (исходящий пакет)**

1. Приложение вызывает send()
- Данные копируются из userspace → kernel space (socket buffer).
- TCP/IP стек формирует заголовки: Ethernet, IP, TCP/UDP.
2. Netfilter hook: OUTPUT
- Firewall может менять пакеты прямо из приложений.
- NAT (SNAT, masquerade) часто применяется здесь.
3. Routing decision
- Ядро смотрит таблицы маршрутизации (RIB/FIB).
- Выбирает интерфейс и next-hop.
4. Traffic Control (qdisc, egress)\
Здесь работают очереди и шейпинг:
- fq_codel (по умолчанию);
- HTB/TBF для ограничения скорости;
- ifb (для ingress shaping).\
_Пример: tc qdisc add dev eth0 root fq_codel._
5. Netfilter hook: POSTROUTING\
Последний шанс изменить пакет (например, SNAT).
6. Драйвер сетевой карты
- skb кладётся в TX ring buffer через DMA.
- NIC берёт пакет и отправляет в сеть.
- Offload (TSO, checksum) может ускорить отправку.

**_Сводная схема хуков ядра:_**
```aiignore
Ingress path:
 [NIC → DMA → skb]
   ↓
 [ingress qdisc] 
   ↓
 [netfilter: PREROUTING]
   ↓
 [Routing decision]
   ├──> Local? → [INPUT] → Socket → App
   └──> Forward? → [FORWARD] → [egress path]

Egress path:
 [App → send()]
   ↓
 [Socket buffer]
   ↓
 [netfilter: OUTPUT]
   ↓
 [Routing decision]
   ↓
 [egress qdisc]
   ↓
 [netfilter: POSTROUTING]
   ↓
 [Driver → DMA → NIC → Wire]

```
_**Важные элементы этого этапа**_\
- Conntrack (connection tracking)\
Таблица активных соединений.
  - Помогает firewall понимать «ESTABLISHED/RELATED».
  - Используется для NAT (сохраняет mapping srcIP:srcPort → newIP:newPort).
- Policy Routing (RPDB)\
Не только ip route, но и ip rule.
  - Выбор таблицы маршрутизации по источнику, fwmark, интерфейсу.
- QoS / Shaping (qdisc)\
Управление очередями. Можно разделять трафик по классам.
- Netfilter/Nftables\
Пакетные фильтры на всех этапах:
  - raw (до conntrack), 
  - mangle (менять поля), 
  - nat (переписывать адреса), 
  - filter (разрешать/запрещать).
  
### 7. Userspace (пространство пользователя)

Если пакет предназначен локальному процессу:
- ядро копирует данные из skb в буфер приложения; 
- приложение получает их через recv() или read().


## Исходящий путь (обратно)

1. Приложение вызывает send() (userspace → kernel). 
2. Данные копируются в socket buffer ядра. 
3. TCP/IP стек формирует заголовки (IP, TCP/UDP). 
4. Netfilter (NAT, firewall). 
5. Ядро кладёт пакет в TX ring buffer NIC через DMA. 
6. NIC берёт данные из памяти и сам отправляет в сеть. 
7. Если нужно, делает checksum offload, segmentation offload (TSO).


## Как это выглядит в виде схемы

```aiignore
Входящий пакет:
[Сеть] → [NIC] → (DMA) → [Ring buffer в RAM]
         → (IRQ → NAPI) → [Драйвер] → [skb]
         → [Netfilter/Маршрутизация/Стек TCP] → [Сокет]
         → [Приложение в userspace]

Исходящий пакет:
[Приложение] → (send) → [Socket buffer ядра]
         → [TCP/IP стек] → [skb]
         → [Netfilter/qdisc] → [Драйвер]
         → (DMA) → [NIC → Сеть]

```

## Как можно «обойти kernel space»?
- DPDK — приложение напрямую читает DMA буферы NIC (минуя ядро).
- XDP/eBPF — пакеты обрабатываются ещё в драйвере, до попадания в skb.
- AF_XDP — гибрид: ядро отдаёт пакеты прямо в userspace-буферы без копирования.

